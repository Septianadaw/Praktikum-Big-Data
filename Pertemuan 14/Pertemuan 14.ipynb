{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f689fe07-0e99-40e9-8320-3a8421db5c6b",
   "metadata": {},
   "source": [
    "Latihan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b018111c-9f2c-4e0e-8114-e750cc44cf0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/29 13:40:40 WARN Utils: Your hostname, septianadaw-VirtualBox resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/11/29 13:40:40 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/29 13:40:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/11/29 13:40:59 WARN Instrumentation: [76d1b842] regParam is zero, which might cause numerical instability and overfitting.\n",
      "25/11/29 13:41:02 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "25/11/29 13:41:03 WARN InstanceBuilder: Failed to load implementation from:dev.ludovic.netlib.lapack.JNILAPACK\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [0.9999999999999992]\n",
      "Intercept: 15.000000000000009\n"
     ]
    }
   ],
   "source": [
    "# Example: Linear Regression with Spark MLlib\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName('MLlib Example').getOrCreate()\n",
    "\n",
    "# Load sample data\n",
    "data = [(1, 5.0, 20.0), (2, 10.0, 25.0), (3, 15.0, 30.0), (4, 20.0, 35.0)]\n",
    "columns = ['ID', 'Feature', 'Target']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Prepare data for modeling\n",
    "assembler = VectorAssembler(inputCols=['Feature'], outputCol='Features')\n",
    "df_transformed = assembler.transform(df)\n",
    "\n",
    "# Train a linear regression model\n",
    "lr = LinearRegression(featuresCol='Features', labelCol='Target')\n",
    "model = lr.fit(df_transformed)\n",
    "\n",
    "# Print model coefficients\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08346d3d-cafa-41bb-8708-7f96097bf854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients: [-12.262057917018774,4.0873522624320255]\n",
      "Intercept: 11.568912714495273\n"
     ]
    }
   ],
   "source": [
    "# Practice: Logistic Regression\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset\n",
    "data = [\n",
    "    (1, Vectors.dense([2.0, 3.0]), 0),\n",
    "    (2, Vectors.dense([1.0, 5.0]), 1),\n",
    "    (3, Vectors.dense([2.5, 4.5]), 1),\n",
    "    (4, Vectors.dense([3.0, 6.0]), 0)\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features', 'Label']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train logistic regression model\n",
    "lr = LogisticRegression(featuresCol='Features', labelCol='Label')\n",
    "model = lr.fit(df)\n",
    "\n",
    "# Display coefficients and summary\n",
    "print(f'Coefficients: {model.coefficients}')\n",
    "print(f'Intercept: {model.intercept}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0031ca8c-1131-443d-9e66-f51435620b01",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: [array([12.5, 12.5]), array([3., 3.])]\n"
     ]
    }
   ],
   "source": [
    "# Practice: KMeans Clustering\n",
    "from pyspark.ml.clustering import KMeans\n",
    "from pyspark.ml.linalg import Vectors\n",
    "\n",
    "# Example dataset (perbaikan)\n",
    "data = [\n",
    "    (1, Vectors.dense([1.0, 1.0])),\n",
    "    (2, Vectors.dense([5.0, 5.0])),\n",
    "    (3, Vectors.dense([10.0, 10.0])),\n",
    "    (4, Vectors.dense([15.0, 15.0]))\n",
    "]\n",
    "\n",
    "columns = ['ID', 'Features']\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Train KMeans clustering model\n",
    "kmeans = KMeans(featuresCol='Features', k=2)\n",
    "model = kmeans.fit(df)\n",
    "\n",
    "# Show cluster centers\n",
    "centers = model.clusterCenters()\n",
    "print(f'Cluster Centers: {centers}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a36b8e-e723-43e0-9133-6d513fe3a244",
   "metadata": {},
   "source": [
    "HOMEWORK\n",
    "\n",
    "1. Load a real-world dataset into Spark and prepare it for machine learning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c61cbc7b-c2bc-4c3d-8a79-aef419442f66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Baris Pertama: \n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Survived|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Embarked|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       0|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| NULL|       S|\n",
      "|          2|       1|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       C|\n",
      "|          3|       1|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282| 7925.0| NULL|       S|\n",
      "|          4|       1|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       S|\n",
      "|          5|       0|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| NULL|       S|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Schema Data: \n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A. Load Dataset & Data Preparation\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Membuat Spark session\n",
    "spark = SparkSession.builder.appName(\"Septiana Pertemuan 14\").getOrCreate()\n",
    "\n",
    "# Load dataset (CSV)\n",
    "df = spark.read.csv(\"Titanic-Dataset.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Menampilkan Data\n",
    "print(\"5 Baris Pertama: \")\n",
    "df.show(5)\n",
    "print(\"Schema Data: \")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4d63fec-2a04-4a3f-92a3-341c3a15780c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah baris sebelum dropDuplicates: 895\n",
      "Jumlah baris duplikat: 4\n",
      "Jumlah baris setelah dropDuplicates: 891\n"
     ]
    }
   ],
   "source": [
    "# B. Drop Duplikasi\n",
    "# Jumlah baris sebelum dropDuplicates\n",
    "rows_before = df.count()\n",
    "print(\"Jumlah baris sebelum dropDuplicates:\", rows_before)\n",
    "\n",
    "# Menghitung jumlah baris unik\n",
    "rows_unique = df.dropDuplicates().count()\n",
    "\n",
    "# Menghitung jumlah duplikasi\n",
    "duplicate_count = rows_before - rows_unique\n",
    "print(\"Jumlah baris duplikat:\", duplicate_count)\n",
    "\n",
    "# Menghapus baris duplikat\n",
    "df = df.dropDuplicates()\n",
    "\n",
    "# Jumlah baris setelah dropDuplicates\n",
    "rows_after = df.count()\n",
    "print(\"Jumlah baris setelah dropDuplicates:\", rows_after)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "71cb69f1-04d0-4680-adb3-9d3b9e73b437",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom yang tersisa: \n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# C. Menghapus kolom yang tidak relevan untuk model machine learning\n",
    "df_clean = df.drop(\"Name\", \"Ticket\", \"Cabin\")\n",
    "print(\"Kolom yang tersisa: \")\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "02427d34-860d-475a-a13e-d20624730961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jumlah Missing Values Setiap Kolom:\n",
      "+-----------+--------+------+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Survived|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+--------+------+---+---+-----+-----+----+--------+\n",
      "|          0|       0|     0|  0|177|    0|    0|   0|       2|\n",
      "+-----------+--------+------+---+---+-----+-----+----+--------+\n",
      "\n",
      "Missing Values setelah imputasi:\n",
      "+-----------+--------+------+---+---+-----+-----+----+--------+\n",
      "|PassengerId|Survived|Pclass|Sex|Age|SibSp|Parch|Fare|Embarked|\n",
      "+-----------+--------+------+---+---+-----+-----+----+--------+\n",
      "|          0|       0|     0|  0|  0|    0|    0|   0|       0|\n",
      "+-----------+--------+------+---+---+-----+-----+----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# D. Missing Values\n",
    "\n",
    "from pyspark.sql.functions import col, when, sum as spark_sum, mean\n",
    "\n",
    "# Mengecek jumlah missing value pada setiap kolom\n",
    "print(\"Jumlah Missing Values Setiap Kolom:\")\n",
    "df_clean.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_clean.columns\n",
    "]).show()\n",
    "\n",
    "# Imputasi \n",
    "# Menghitung mean kolom Age untuk mengisi nilai kosong\n",
    "mean_age = df_clean.select(mean(\"Age\")).first()[0]\n",
    "df_clean = df_clean.fillna({\"Age\": mean_age, \"Embarked\": \"S\"})\n",
    "\n",
    "# Mengecek kembali jumlah missing value setelah proses imputasi\n",
    "print(\"Missing Values setelah imputasi:\")\n",
    "df_clean.select([\n",
    "    spark_sum(when(col(c).isNull(), 1).otherwise(0)).alias(c)\n",
    "    for c in df_clean.columns\n",
    "]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "89735ddf-2bc4-44d6-adab-4323a9d1fdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# E. Encoding Kolom Kategorikal (String ke Numeric)\n",
    "# Import StringIndexer untuk mengubah string menjadi angka\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder\n",
    "\n",
    "# Mengubah kolom Sex menjadi angka\n",
    "indexer_sex = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "\n",
    "# Mengubah kolom Embarked menjadi angka\n",
    "indexer_embarked = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")\n",
    "\n",
    "# OneHotEncoder untuk membuat vektor biner\n",
    "encoder = OneHotEncoder(\n",
    "    inputCols=[\"SexIndex\", \"EmbarkedIndex\"],\n",
    "    outputCols=[\"SexVec\", \"EmbarkedVec\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "62593d59-dcd0-4ec3-9e7a-1de64d92d26c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F. Gabungkan Semua Fitur Menjadi Vector\n",
    "\n",
    "# Import VectorAssembler untuk menggabungkan fitur\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "# Daftar kolom fitur yang digunakan\n",
    "feature_cols = [\"Pclass\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"SexVec\", \"EmbarkedVec\"]\n",
    "\n",
    "# Menggabungkan semua fitur menjadi satu kolom \"features\"\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c5b15b-7bbe-4ff6-9fd3-888935f98ed9",
   "metadata": {},
   "source": [
    "2. Build a Classification Model Using Spark MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e884b018-db77-4999-a818-6c7d278134c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+--------------------+\n",
      "|Survived|prediction|         probability|\n",
      "+--------+----------+--------------------+\n",
      "|       1|       0.0|[0.56188488508122...|\n",
      "|       0|       0.0|[0.75774398187318...|\n",
      "|       1|       0.0|[0.55301519130739...|\n",
      "|       0|       0.0|[0.78046049430939...|\n",
      "|       1|       1.0|[0.47207410845731...|\n",
      "+--------+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Import RandomForestClassifier untuk klasifikasi\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Membuat model Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"Survived\",\n",
    "    featuresCol=\"features\"\n",
    ")\n",
    "\n",
    "# Membuat Pipeline untuk menggabungkan seluruh tahap preprocessing dan model\n",
    "pipeline = Pipeline(stages=[indexer_sex, indexer_embarked, encoder, assembler, rf])\n",
    "\n",
    "# Split Train dan Test: Membagi data 80% training dan 20% testing\n",
    "train_df, test_df = df_clean.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train Model: Melatih model menggunakan data training\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Membuat prediksi pada data test\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "# Menampilkan beberapa hasil prediksi\n",
    "predictions.select(\"Survived\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97b873af-2acc-48c1-aa19-9b250435a110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hasil Evaluasi Model: \n",
      "Accuracy : 0.8275862068965517\n",
      "F1 Score : 0.8227029342471621\n",
      "Precision: 0.8491157127621756\n",
      "Recall   : 0.8275862068965518\n"
     ]
    }
   ],
   "source": [
    "# Evaluasi Model (Accuracy, F1, Precision, Recall)\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Membuat Evaluator\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"Survived\",\n",
    "    predictionCol=\"prediction\"\n",
    ")\n",
    "\n",
    "# Accuracy, F1 Score, Precision, Recall\n",
    "accuracy = evaluator.evaluate(predictions, {evaluator.metricName: \"accuracy\"})\n",
    "f1 = evaluator.evaluate(predictions, {evaluator.metricName: \"f1\"})\n",
    "precision = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Menampilkan hasil evaluasi\n",
    "print(\"Hasil Evaluasi Model: \")\n",
    "print(\"Accuracy :\", accuracy)\n",
    "print(\"F1 Score :\", f1)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall   :\", recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a33c10e0-cb5f-4116-a525-de0aba7d0a30",
   "metadata": {},
   "source": [
    "3. Hyperparameter Tuning (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e182f4c-bd54-46b7-80b9-88d2071230e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1676:=========================================>          (160 + 6) / 200]"
     ]
    }
   ],
   "source": [
    "# Import builder untuk membuat grid parameter\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "# Daftar kombinasi parameter yang akan diuji\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(rf.maxDepth, [3, 5, 7])\n",
    "             .addGrid(rf.numTrees, [50, 100])\n",
    "             .build())\n",
    "\n",
    "# Membuat CrossValidator\n",
    "cv = CrossValidator(\n",
    "    estimator=pipeline,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator,\n",
    "    numFolds=5\n",
    ")\n",
    "\n",
    "# Melatih Model Dengan Cross Validation\n",
    "cv_model = cv.fit(train_df)\n",
    "\n",
    "# Prediksi Menggunakan Model\n",
    "cv_predictions = cv_model.transform(test_df)\n",
    "\n",
    "# Evaluasi Model\n",
    "# Menghitung accuracy, F1, Precision, Recall terbaik\n",
    "best_accuracy = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"accuracy\"})\n",
    "best_f1 = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"f1\"})\n",
    "precision = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"weightedPrecision\"})\n",
    "recall = evaluator.evaluate(cv_predictions, {evaluator.metricName: \"weightedRecall\"})\n",
    "\n",
    "# Menampilkan hasil evaluasi terbaik\n",
    "print(\"Best CV Accuracy :\", best_accuracy)\n",
    "print(\"Best CV F1       :\", best_f1)\n",
    "print(\"Best CV Precision:\", precision)\n",
    "print(\"Best CV Recall   :\", recall)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
